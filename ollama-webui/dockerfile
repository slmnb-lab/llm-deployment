# 使用官方镜像作为基础镜像
FROM ghcr.io/open-webui/open-webui:ollama

# 设置工作目录
WORKDIR /app/backend

# 复制检查脚本
COPY pullmodel.sh /app/backend/
RUN chmod +x /app/backend/pullmodel.sh

# 创建数据持久化的卷
# VOLUME ["/root/.ollama", "/app/backend/data"]

# 暴露端口
EXPOSE 8080

# 设置默认模型环境变量
ENV OLLAMA_HOST=0.0.0.0:11434
ENV USE_OLLAMA=true
ENV USE_CUDA=true

# 设置默认模型
# 如果没有设置环境变量 DEFAULT_MODEL，则使用 qwen3:30b-a3b 作为默认模型
ENV DEFAULT_MODEL=${DEFAULT_MODEL:-qwen3:30b-a3b}

# 预先下载模型
RUN ollama serve & \
    sleep 10 && \
    ollama pull ${DEFAULT_MODEL}

# 设置容器启动命令
CMD ["/app/backend/pullmodel.sh"]